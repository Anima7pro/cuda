{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5b16ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è‡ªå®šä¹‰ %%cuda å·²æ¿€æ´»ï¼ç°åœ¨ä½ å¯ä»¥ç›´æ¥å†™ä»£ç äº†ï¼Œæ— éœ€ä»»ä½•å‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "@register_cell_magic\n",
    "def cuda(line, cell):\n",
    "    # 1. æŠŠ Cell é‡Œçš„ä»£ç ä¿å­˜ä¸ºæ–‡ä»¶\n",
    "    filename = \"cuda_code.cu\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(cell)\n",
    "\n",
    "    # 2. ç¼–è¯‘å‘½ä»¤ (è‡ªåŠ¨åŠ ä¸Šäº† -arch=sm_75 ä¿®å¤ä½ çš„æŠ¥é”™)\n",
    "    # è¿™é‡Œçš„ -arch=sm_75 æ˜¯ä¸“é—¨é’ˆå¯¹ Colab T4 æ˜¾å¡çš„\n",
    "    compile_cmd = \"/usr/local/cuda/bin/nvcc -arch=sm_75 -o cuda_code cuda_code.cu\"\n",
    "    \n",
    "    # 3. æ‰§è¡Œç¼–è¯‘\n",
    "    print(f\"ğŸ”¨ Compiling with: {compile_cmd}\")\n",
    "    result = subprocess.run(compile_cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(\"âŒ Compilation Failed!\")\n",
    "        print(result.stderr)\n",
    "        return\n",
    "\n",
    "    # 4. è¿è¡Œç¼–è¯‘å¥½çš„ç¨‹åº\n",
    "    print(\"ğŸš€ Running...\")\n",
    "    run_cmd = \"./cuda_code\"\n",
    "    run_result = subprocess.run(run_cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    # 5. è¾“å‡ºç»“æœ\n",
    "    print(run_result.stdout)\n",
    "    if run_result.stderr:\n",
    "        print(\"Runtime Errors:\", run_result.stderr)\n",
    "\n",
    "print(\"âœ… è‡ªå®šä¹‰ %%cuda å·²æ¿€æ´»ï¼ç°åœ¨ä½ å¯ä»¥ç›´æ¥å†™ä»£ç äº†ï¼Œæ— éœ€ä»»ä½•å‚æ•°ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1f410eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
      "âœ… æ£€æµ‹åˆ° GPU: Tesla T4\n",
      "âœ… ç¼–è¯‘æˆåŠŸï¼\n",
      "éªŒè¯ç»“æœ: True\n"
     ]
    }
   ],
   "source": [
    "# 1. å®‰è£…å¿…è¦çš„æ„å»ºå·¥å…·\n",
    "!pip install ninja\n",
    "\n",
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "import os\n",
    "\n",
    "# 2. æ¸…ç†ç¼“å­˜ (é˜²æ­¢æ—§çš„é”™è¯¯æ„å»ºå¹²æ‰°)\n",
    "!rm -rf /root/.cache/torch_extensions/\n",
    "\n",
    "# 3. å†æ¬¡ç¡®è®¤ CUDA å¯ç”¨\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"âŒ é”™è¯¯: æœªæ£€æµ‹åˆ° GPUï¼Œè¯·åœ¨ Colab èœå• 'Runtime' -> 'Change runtime type' ä¸­é€‰æ‹© GPUã€‚\")\n",
    "else:\n",
    "    print(f\"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 4. å®šä¹‰ä»£ç \n",
    "cuda_src = \"\"\"\n",
    "__global__ void square_matrix_kernel(const float* input, float* output, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        output[idx] = input[idx] * input[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void square_matrix(torch::Tensor input, torch::Tensor output) {\n",
    "    int size = input.numel();\n",
    "    const int threads = 256;\n",
    "    const int blocks = (size + threads - 1) / threads;\n",
    "    square_matrix_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cpp_src = \"void square_matrix(torch::Tensor input, torch::Tensor output);\"\n",
    "\n",
    "# 5. ç¼–è¯‘ (å…³é”®ä¿®æ”¹ï¼šåŠ å…¥äº† verbose=True)\n",
    "try:\n",
    "    square_module = load_inline(\n",
    "        name='square_extension',\n",
    "        cpp_sources=cpp_src,\n",
    "        cuda_sources=cuda_src,\n",
    "        functions=['square_matrix'],\n",
    "        with_cuda=True,\n",
    "        extra_cuda_cflags=[\"-O3\"],\n",
    "        verbose=True  # ğŸ”¥ å…³é”®ï¼šè¿™ä¼šæŠŠåº•å±‚çš„ç¼–è¯‘æ—¥å¿—æ‰“å°å‡ºæ¥\n",
    "    )\n",
    "    print(\"âœ… ç¼–è¯‘æˆåŠŸï¼\")\n",
    "\n",
    "    # æµ‹è¯•\n",
    "    x = torch.randn(1000).cuda()\n",
    "    y = torch.zeros_like(x)\n",
    "    square_module.square_matrix(x, y)\n",
    "    print(f\"éªŒè¯ç»“æœ: {torch.allclose(y, x*x)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nâŒ ç¼–è¯‘ä¾ç„¶å¤±è´¥ã€‚è¯·æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†æ—¥å¿— (Look at the log above)ã€‚\")\n",
    "    # å¦‚æœæ˜¯å› ä¸ºç¯å¢ƒé‡Œæ²¡æœ‰ gcc/g++ï¼Œæœ‰æ—¶éœ€è¦å®‰è£… (Colabé€šå¸¸è‡ªå¸¦ï¼Œä½†ä»¥é˜²ä¸‡ä¸€)\n",
    "    # print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27babc1a",
   "metadata": {},
   "source": [
    "# Naive VS Tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1725e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹å·²æ¸…ç†ç¼–è¯‘ç¼“å­˜\n",
      "ğŸ” æ­£åœ¨ä» /content è¯»å– CUDA æºç ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/GEMM/NaiveGEMM.cu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2853298477.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ” æ­£åœ¨ä» {root} è¯»å– CUDA æºç ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m cuda_source = ((root / \"GEMM\" / \"NaiveGEMM.cu\").read_text() + \"\\n\"\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"GEMM\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"TiledGEMM.cu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"GEMM\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"RegisterTiledGEMM.cu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/GEMM/NaiveGEMM.cu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from torch.utils.cpp_extension import load\n",
    "import os\n",
    "import shutil\n",
    "import sysconfig\n",
    "from pathlib import Path\n",
    "\n",
    "cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache/torch_extensions\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "print(\"ğŸ§¹å·²æ¸…ç†ç¼–è¯‘ç¼“å­˜\")\n",
    "\n",
    "# è·å– Python.h æ‰€åœ¨çš„çœŸå®è·¯å¾„\n",
    "# python_include = sysconfig.get_path('include')\n",
    "# print(f\"ğŸ” Python.h åº”è¯¥åœ¨è¿™é‡Œ: {python_include}\")\n",
    "\n",
    "# å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©ç¼–è¯‘å™¨ä¸€å®šèƒ½çœ‹åˆ°å®ƒ\n",
    "# os.environ['CPLUS_INCLUDE_PATH'] = python_include + os.pathsep + os.environ.get('CPLUS_INCLUDE_PATH', '')\n",
    "# print(\"âœ… å·²å¼ºåˆ¶æ³¨å…¥å¤´æ–‡ä»¶è·¯å¾„\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. CUDA æºç \n",
    "# --------------------------------------------------\n",
    "root = Path.cwd()\n",
    "print(f\"ğŸ” æ­£åœ¨ä» {root} è¯»å– CUDA æºç ...\")\n",
    "cuda_source = ((root / \"GEMM\" / \"NaiveGEMM.cu\").read_text() + \"\\n\"\n",
    "                + (root / \"GEMM\" / \"TiledGEMM.cu\").read_text() + \"\\n\" \n",
    "                + (root / \"GEMM\" / \"RegisterTiledGEMM.cu\").read_text() + \"\\n\"\n",
    "                + (root / \"GEMM\" / \"HOST\" /\"NaiveGEMM.cu\").read_text() + \"\\n\"\n",
    "                + (root / \"GEMM\" / \"HOST\" /\"TiledGEMM.cu\").read_text() + \"\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "cpp_source = \"\"\"\n",
    "void run_sgemm_naive(torch::Tensor A, torch::Tensor B, torch::Tensor C);\n",
    "void run_sgemm_tiled(torch::Tensor A, torch::Tensor B, torch::Tensor C);\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. ç¼–è¯‘æ¨¡å—\n",
    "# ------------------------------------------------------------------\n",
    "print(\" æ­£åœ¨ç¼–è¯‘ CUDA ç®—å­...\")\n",
    "gemm_module = load_inline(\n",
    "    name='sgemm_comparison',\n",
    "    cpp_sources=cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=['run_sgemm_naive', 'run_sgemm_tiled'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O3\"]\n",
    ")\n",
    "print(\" ç¼–è¯‘å®Œæˆï¼\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. æ€§èƒ½æµ‹è¯• (Benchmark)\n",
    "# ------------------------------------------------------------------\n",
    "# è®¾ç½®çŸ©é˜µå¤§å° (2048x2048 æ˜¯ä¸€ä¸ªæ¯”è¾ƒèƒ½æ˜¾ç°å·®è·çš„å¤§å°)\n",
    "M, N, K = 2048, 2048, 2048\n",
    "A = torch.randn(M, K).cuda()\n",
    "B = torch.randn(K, N).cuda()\n",
    "C_naive = torch.zeros(M, N).cuda()\n",
    "C_tiled = torch.zeros(M, N).cuda()\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼šè®¡æ—¶\n",
    "def benchmark(func, name, *args):\n",
    "    # é¢„çƒ­\n",
    "    func(*args)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(10): # è·‘ 10 æ¬¡å–å¹³å‡\n",
    "        func(*args)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    avg_time = start.elapsed_time(end) / 10.0\n",
    "    print(f\"[{name}] å¹³å‡è€—æ—¶: {avg_time:.2f} ms\")\n",
    "    return avg_time\n",
    "\n",
    "print(f\"\\n å¼€å§‹æµ‹è¯• (çŸ©é˜µå¤§å°: {M}x{N}x{K})...\")\n",
    "\n",
    "# 1. è·‘ Naive\n",
    "time_naive = benchmark(gemm_module.run_sgemm_naive, \"Naive GEMM\", A, B, C_naive)\n",
    "\n",
    "# 2. è·‘ Tiled\n",
    "time_tiled = benchmark(gemm_module.run_sgemm_tiled, \"Tiled GEMM\", A, B, C_tiled)\n",
    "\n",
    "# 3. éªŒè¯æ­£ç¡®æ€§\n",
    "is_correct = torch.allclose(C_naive, C_tiled, atol=1e-2)\n",
    "print(f\"\\n ç»“æœåŒ¹é…? {is_correct}\")\n",
    "\n",
    "# 4. è®¡ç®—åŠ é€Ÿæ¯”\n",
    "if is_correct:\n",
    "    print(f\"âš¡ åŠ é€Ÿæ¯”: {time_naive / time_tiled:.2f}x (Tiled æ¯” Naive å¿«äº†è¿™ä¹ˆå¤š)\")\n",
    "else:\n",
    "    print(\" ç»“æœä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥ä»£ç é€»è¾‘ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e9d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Python.h åº”è¯¥åœ¨è¿™é‡Œ: /usr/include/python3.12\n",
      "âœ… å·²å¼ºåˆ¶æ³¨å…¥å¤´æ–‡ä»¶è·¯å¾„ï¼Œè¯·é‡è¯•ç¼–è¯‘ã€‚\n",
      "ğŸ” æ­£åœ¨å°è¯•æ‰‹åŠ¨ç¼–è¯‘ .o æ–‡ä»¶...\n",
      "âœ… æ‰‹åŠ¨ç¼–è¯‘æˆåŠŸï¼nvcc ç¼–è¯‘å™¨æ˜¯æ­£å¸¸çš„ã€‚\n",
      "é—®é¢˜å‡ºåœ¨ PyTorch çš„ load_inline å°è£…ä¸Šã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "from torch.utils.cpp_extension import include_paths, library_paths\n",
    "\n",
    "# è·å– Python.h æ‰€åœ¨çš„çœŸå®è·¯å¾„\n",
    "python_include = sysconfig.get_path('include')\n",
    "print(f\"ğŸ” Python.h åº”è¯¥åœ¨è¿™é‡Œ: {python_include}\")\n",
    "\n",
    "# å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©ç¼–è¯‘å™¨ä¸€å®šèƒ½çœ‹åˆ°å®ƒ\n",
    "os.environ['CPLUS_INCLUDE_PATH'] = python_include + os.pathsep + os.environ.get('CPLUS_INCLUDE_PATH', '')\n",
    "print(\"âœ… å·²å¼ºåˆ¶æ³¨å…¥å¤´æ–‡ä»¶è·¯å¾„ï¼Œè¯·é‡è¯•ç¼–è¯‘ã€‚\")\n",
    "# 1. å‡†å¤‡ä¸€æ®µæœ€ç®€å•çš„ CUDA ä»£ç \n",
    "code = \"\"\"\n",
    "#include <torch/extension.h>\n",
    "__global__ void test_kernel(float* x) { *x = 1.0; }\n",
    "void test_func(torch::Tensor x) { test_kernel<<<1,1>>>(x.data_ptr<float>()); }\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(\"test_func\", &test_func); }\n",
    "\"\"\"\n",
    "\n",
    "# 2. å†™å…¥æ–‡ä»¶\n",
    "with open(\"test_manual.cu\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "# 3. æ‹¼è£…ç¼–è¯‘å‘½ä»¤ (æ¨¡æ‹Ÿ PyTorch çš„è¡Œä¸º)\n",
    "# è·å– PyTorch çš„å¤´æ–‡ä»¶è·¯å¾„\n",
    "includes = [f\"-I{p}\" for p in include_paths()]\n",
    "# è·å– PyTorch çš„åº“è·¯å¾„\n",
    "libs = [f\"-L{p}\" for p in library_paths()]\n",
    "\n",
    "# å…³é”®ï¼šæ‰‹åŠ¨è°ƒç”¨ nvcc\n",
    "cmd = [\n",
    "    \"nvcc\", \"-c\", \"test_manual.cu\", \"-o\", \"test_manual.o\",\n",
    "    \"-x\", \"cu\", \"-Xcompiler\", \"-fPIC\", \"-std=c++17\",\n",
    "    \"--expt-relaxed-constexpr\"\n",
    "] + includes\n",
    "\n",
    "print(\"ğŸ” æ­£åœ¨å°è¯•æ‰‹åŠ¨ç¼–è¯‘ .o æ–‡ä»¶...\")\n",
    "res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if res.returncode != 0:\n",
    "    print(\"âŒ æ‰‹åŠ¨ç¼–è¯‘å¤±è´¥ï¼çœŸæ­£çš„é”™è¯¯å¦‚ä¸‹ï¼š\")\n",
    "    print(\"=\"*40)\n",
    "    print(res.stderr)\n",
    "    print(\"=\"*40)\n",
    "else:\n",
    "    print(\"âœ… æ‰‹åŠ¨ç¼–è¯‘æˆåŠŸï¼nvcc ç¼–è¯‘å™¨æ˜¯æ­£å¸¸çš„ã€‚\")\n",
    "    print(\"é—®é¢˜å‡ºåœ¨ PyTorch çš„ load_inline å°è£…ä¸Šã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
