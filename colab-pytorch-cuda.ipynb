{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5b16ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è‡ªå®šä¹‰ %%cuda å·²æ¿€æ´»ï¼ç°åœ¨ä½ å¯ä»¥ç›´æ¥å†™ä»£ç äº†ï¼Œæ— éœ€ä»»ä½•å‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "@register_cell_magic\n",
    "def cuda(line, cell):\n",
    "    # 1. æŠŠ Cell é‡Œçš„ä»£ç ä¿å­˜ä¸ºæ–‡ä»¶\n",
    "    filename = \"cuda_code.cu\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(cell)\n",
    "\n",
    "    # 2. ç¼–è¯‘å‘½ä»¤ (è‡ªåŠ¨åŠ ä¸Šäº† -arch=sm_75 ä¿®å¤ä½ çš„æŠ¥é”™)\n",
    "    # è¿™é‡Œçš„ -arch=sm_75 æ˜¯ä¸“é—¨é’ˆå¯¹ Colab T4 æ˜¾å¡çš„\n",
    "    compile_cmd = \"/usr/local/cuda/bin/nvcc -arch=sm_75 -o cuda_code cuda_code.cu\"\n",
    "    \n",
    "    # 3. æ‰§è¡Œç¼–è¯‘\n",
    "    print(f\"ğŸ”¨ Compiling with: {compile_cmd}\")\n",
    "    result = subprocess.run(compile_cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(\"âŒ Compilation Failed!\")\n",
    "        print(result.stderr)\n",
    "        return\n",
    "\n",
    "    # 4. è¿è¡Œç¼–è¯‘å¥½çš„ç¨‹åº\n",
    "    print(\"ğŸš€ Running...\")\n",
    "    run_cmd = \"./cuda_code\"\n",
    "    run_result = subprocess.run(run_cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    # 5. è¾“å‡ºç»“æœ\n",
    "    print(run_result.stdout)\n",
    "    if run_result.stderr:\n",
    "        print(\"Runtime Errors:\", run_result.stderr)\n",
    "\n",
    "print(\"âœ… è‡ªå®šä¹‰ %%cuda å·²æ¿€æ´»ï¼ç°åœ¨ä½ å¯ä»¥ç›´æ¥å†™ä»£ç äº†ï¼Œæ— éœ€ä»»ä½•å‚æ•°ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1f410eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
      "âœ… æ£€æµ‹åˆ° GPU: Tesla T4\n",
      "âœ… ç¼–è¯‘æˆåŠŸï¼\n",
      "éªŒè¯ç»“æœ: True\n"
     ]
    }
   ],
   "source": [
    "# 1. å®‰è£…å¿…è¦çš„æ„å»ºå·¥å…·\n",
    "!pip install ninja\n",
    "\n",
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "import os\n",
    "\n",
    "# 2. æ¸…ç†ç¼“å­˜ (é˜²æ­¢æ—§çš„é”™è¯¯æ„å»ºå¹²æ‰°)\n",
    "!rm -rf /root/.cache/torch_extensions/\n",
    "\n",
    "# 3. å†æ¬¡ç¡®è®¤ CUDA å¯ç”¨\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"âŒ é”™è¯¯: æœªæ£€æµ‹åˆ° GPUï¼Œè¯·åœ¨ Colab èœå• 'Runtime' -> 'Change runtime type' ä¸­é€‰æ‹© GPUã€‚\")\n",
    "else:\n",
    "    print(f\"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 4. å®šä¹‰ä»£ç \n",
    "cuda_src = \"\"\"\n",
    "__global__ void square_matrix_kernel(const float* input, float* output, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        output[idx] = input[idx] * input[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void square_matrix(torch::Tensor input, torch::Tensor output) {\n",
    "    int size = input.numel();\n",
    "    const int threads = 256;\n",
    "    const int blocks = (size + threads - 1) / threads;\n",
    "    square_matrix_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cpp_src = \"void square_matrix(torch::Tensor input, torch::Tensor output);\"\n",
    "\n",
    "# 5. ç¼–è¯‘ (å…³é”®ä¿®æ”¹ï¼šåŠ å…¥äº† verbose=True)\n",
    "try:\n",
    "    square_module = load_inline(\n",
    "        name='square_extension',\n",
    "        cpp_sources=cpp_src,\n",
    "        cuda_sources=cuda_src,\n",
    "        functions=['square_matrix'],\n",
    "        with_cuda=True,\n",
    "        extra_cuda_cflags=[\"-O3\"],\n",
    "        verbose=True  # ğŸ”¥ å…³é”®ï¼šè¿™ä¼šæŠŠåº•å±‚çš„ç¼–è¯‘æ—¥å¿—æ‰“å°å‡ºæ¥\n",
    "    )\n",
    "    print(\"âœ… ç¼–è¯‘æˆåŠŸï¼\")\n",
    "\n",
    "    # æµ‹è¯•\n",
    "    x = torch.randn(1000).cuda()\n",
    "    y = torch.zeros_like(x)\n",
    "    square_module.square_matrix(x, y)\n",
    "    print(f\"éªŒè¯ç»“æœ: {torch.allclose(y, x*x)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nâŒ ç¼–è¯‘ä¾ç„¶å¤±è´¥ã€‚è¯·æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†æ—¥å¿— (Look at the log above)ã€‚\")\n",
    "    # å¦‚æœæ˜¯å› ä¸ºç¯å¢ƒé‡Œæ²¡æœ‰ gcc/g++ï¼Œæœ‰æ—¶éœ€è¦å®‰è£… (Colabé€šå¸¸è‡ªå¸¦ï¼Œä½†ä»¥é˜²ä¸‡ä¸€)\n",
    "    # print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27babc1a",
   "metadata": {},
   "source": [
    "# Naive VS Tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0e1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cuda'...\n",
      "remote: Enumerating objects: 34, done.\u001b[K\n",
      "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 34 (delta 6), reused 34 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (34/34), 16.32 KiB | 2.04 MiB/s, done.\n",
      "Resolving deltas: 100% (6/6), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Anima7pro/cuda.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50ddc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1Dreduce\u001b[0m/                 \u001b[01;34mGEMM\u001b[0m/                  notbook.ipynb  \u001b[01;34mSoftmax\u001b[0m/\n",
      "colab-pytorch-cuda.ipynb  \u001b[01;34mMatrixMultiplication\u001b[0m/  README.md      \u001b[01;34mVectorAddition\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1725e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹å·²æ¸…ç†ç¼–è¯‘ç¼“å­˜\n",
      "ğŸ” æ­£åœ¨ä» /content/cuda/GEMM è¯»å– CUDA æºç ...\n",
      " æ­£åœ¨ç¼–è¯‘ CUDA ç®—å­...\n",
      " ç¼–è¯‘å®Œæˆï¼\n",
      "\n",
      " å¼€å§‹æµ‹è¯• (çŸ©é˜µå¤§å°: 2048x2048x2048)...\n",
      "[Naive GEMM] å¹³å‡è€—æ—¶: 52.97 ms\n",
      "[Tiled GEMM] å¹³å‡è€—æ—¶: 30.67 ms\n",
      "\n",
      " ç»“æœåŒ¹é…? True\n",
      "âš¡ åŠ é€Ÿæ¯”: 1.73x (Tiled æ¯” Naive å¿«äº†è¿™ä¹ˆå¤š)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from torch.utils.cpp_extension import load\n",
    "import os\n",
    "import shutil\n",
    "import sysconfig\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache/torch_extensions\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "print(\"ğŸ§¹å·²æ¸…ç†ç¼–è¯‘ç¼“å­˜\")\n",
    "\n",
    "# è·å– Python.h æ‰€åœ¨çš„çœŸå®è·¯å¾„\n",
    "# python_include = sysconfig.get_path('include')\n",
    "# print(f\"ğŸ” Python.h åº”è¯¥åœ¨è¿™é‡Œ: {python_include}\")\n",
    "\n",
    "# å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©ç¼–è¯‘å™¨ä¸€å®šèƒ½çœ‹åˆ°å®ƒ\n",
    "# os.environ['CPLUS_INCLUDE_PATH'] = python_include + os.pathsep + os.environ.get('CPLUS_INCLUDE_PATH', '')\n",
    "# print(\"âœ… å·²å¼ºåˆ¶æ³¨å…¥å¤´æ–‡ä»¶è·¯å¾„\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. CUDA æºç \n",
    "# --------------------------------------------------\n",
    "root = Path.cwd()\n",
    "root = root / \"cuda\" / \"GEMM\"\n",
    "print(f\"ğŸ” æ­£åœ¨ä» {root} è¯»å– CUDA æºç ...\")\n",
    "cuda_source = ((root / \"NaiveGEMM.cu\").read_text() + \"\\n\"\n",
    "                + (root / \"TiledKernelGEMM.cu\").read_text() + \"\\n\" \n",
    "                + (root / \"RegisterTiledGEMM.cu\").read_text() + \"\\n\"\n",
    "                + (root / \"HOST\" /\"NaiveGEMM.cu\").read_text() + \"\\n\"\n",
    "                + (root / \"HOST\" /\"TiledKernelGEMM.cu\").read_text() + \"\\n\"\n",
    "                + (root / \"HOST\" /\"RegisterTiledGEMM.cu\").read_text()\n",
    "                )\n",
    "\n",
    "\n",
    "cpp_source = \"\"\"\n",
    "void run_sgemm_naive(torch::Tensor A, torch::Tensor B, torch::Tensor C);\n",
    "void run_sgemm_tiled(torch::Tensor A, torch::Tensor B, torch::Tensor C);\n",
    "void run_sgemm_RegisterTiled(torch::Tensor A, torch::Tensor B, torch::Tensor C);\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. ç¼–è¯‘æ¨¡å—\n",
    "# ------------------------------------------------------------------\n",
    "print(\" æ­£åœ¨ç¼–è¯‘ CUDA ç®—å­...\")\n",
    "gemm_module = load_inline(\n",
    "    name='sgemm_comparison',\n",
    "    cpp_sources=cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=['run_sgemm_naive', 'run_sgemm_tiled', 'run_sgemm_RegisterTiled'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O3\"]\n",
    ")\n",
    "print(\" ç¼–è¯‘å®Œæˆï¼\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. æ€§èƒ½æµ‹è¯• (Benchmark)\n",
    "# ------------------------------------------------------------------\n",
    "# è®¾ç½®çŸ©é˜µå¤§å° (2048x2048 æ˜¯ä¸€ä¸ªæ¯”è¾ƒèƒ½æ˜¾ç°å·®è·çš„å¤§å°)\n",
    "M, N, K = 4096, 4096, 4096\n",
    "A = torch.randn(M, K).cuda()\n",
    "B = torch.randn(K, N).cuda()\n",
    "C_naive = torch.zeros(M, N).cuda()\n",
    "C_tiled = torch.zeros(M, N).cuda()\n",
    "C_RegisterTiled = torch.zeros(M, N).cuda()\n",
    "\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼šè®¡æ—¶\n",
    "def benchmark(func, name, *args):\n",
    "    # é¢„çƒ­\n",
    "    func(*args)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(10): # è·‘ 10 æ¬¡å–å¹³å‡\n",
    "        func(*args)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    avg_time = start.elapsed_time(end) / 10.0\n",
    "    print(f\"[{name}] å¹³å‡è€—æ—¶: {avg_time:.2f} ms\")\n",
    "    return avg_time\n",
    "\n",
    "print(f\"\\n å¼€å§‹æµ‹è¯• (çŸ©é˜µå¤§å°: {M}x{N}x{K})...\")\n",
    "\n",
    "# 1. è·‘ Naive\n",
    "time_naive = benchmark(gemm_module.run_sgemm_naive, \"Naive GEMM\", A, B, C_naive)\n",
    "\n",
    "# 2. è·‘ Tiled\n",
    "time_tiled = benchmark(gemm_module.run_sgemm_tiled, \"Tiled GEMM\", A, B, C_tiled)\n",
    "\n",
    "# 3. è·‘ RegisterTiled\n",
    "time_RegisterTiled = benchmark(gemm_module.run_sgemm_RegisterTiled, \"RegisterTiled GEMM\", A, B, C_RegisterTiled)\n",
    "\n",
    "\n",
    "# 3. éªŒè¯æ­£ç¡®æ€§\n",
    "is_correct = torch.allclose(C_naive, C_tiled, C_RegisterTiled, atol=1e-3)\n",
    "print(f\"\\n ç»“æœåŒ¹é…? {is_correct}\")\n",
    "\n",
    "# 4. è®¡ç®—åŠ é€Ÿæ¯”\n",
    "if is_correct:\n",
    "    print(f\" åŠ é€Ÿæ¯”: {time_naive / time_tiled:.2f}x \")\n",
    "else:\n",
    "    print(\" ç»“æœä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥ä»£ç é€»è¾‘ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4f31c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
